{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, login to huggingface, which has the dataset and important classes that will be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d61793f3e8414f8547ba9d38db9f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dataset from wikipedia pages (2023) (English)\n",
    "\n",
    "Luckly, it only needs to be download once, and it will be saved in cache!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527b3db687354d8c90426affe861bf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19946c08029b48849cc70b8ba2aafdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 6407814\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wikipedia = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "\n",
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 6407814\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove useless columns\n",
    "raw_dataset = wikipedia[\"train\"].remove_columns([col for col in wikipedia[\"train\"].column_names if col != \"text\"])\n",
    "\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a tokenizer, starting from a pre-trained one from hugging face\n",
    "\n",
    "It is important to train a tokenizer, because it will affect it's ability to interact and interpret the corpus provided !\n",
    "\n",
    "It will contain the following special tokens:\n",
    "\n",
    "    [UNK]: Unknown\n",
    "    [SEP]: Separator (for sequences)\n",
    "    [PAD]: Padding (to fill empty spots)\n",
    "    [CLS]: Classification (initial token used as classifier)\n",
    "    [MASK]: Masking (token that represents a masked token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train once, save it on hub for later loading"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# create a python generator to dynamically load the data, one batch at a time\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(raw_dataset), batch_size)):\n",
    "        yield raw_dataset[i : i + batch_size][\"text\"]\n",
    "\n",
    "# load a tokenizer from existing one to re-use special tokens\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bert_tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# repository id for saving the tokenizer in hugging face hub\n",
    "tokenizer_id=\"bert-base-uncased-wallace\"\n",
    "\n",
    "# push the tokenizer\n",
    "bert_tokenizer.push_to_hub(tokenizer_id)\n",
    "\n",
    "# save locally too\n",
    "bert_tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 25281, 26019, 25281, 43, 25281, 25041, 43, 28445, 24988, 25281, 25281, 43, 25281, 35, 3]\n",
      "['[CLS]', 'can', 'you', 'can', 'a', 'can', 'as', 'a', 'cann', '##er', 'can', 'can', 'a', 'can', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample = '''\n",
    "Can you can a can as a canner can can a can?\n",
    "'''\n",
    "\n",
    "encoding = tokenizer.encode(sample)\n",
    "\n",
    "print(encoding)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(encoding))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
